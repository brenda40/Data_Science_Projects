{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brenda Woodard\n",
    "# Assignment 5\n",
    "\n",
    "# Goal: The goal of this assignment is to give you the opportunity to build the Naïve Bayes Algorithm from scratch as well\n",
    "# as using tools built into sklearn. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import statsmodels.tools.tools as stattools\n",
    "import html\n",
    "from scipy import stats\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "# In sklearn, there are different types of Naive Bayes constructors for fitting Naïve Bayes models, dependent on the nature \n",
    "# of the data. \n",
    "# For example:\n",
    "#     MultinomialNB() is used for text classification when data is represented as a feature vector \n",
    "#     ComplementNB() is an adaptation of the standard  MultinomialNB() for imbalanced data\n",
    "#     GaussianNB() is used if the features are assumed numerical & are assumed to follow a Gaussian or normal distribution \n",
    "#     BernoulliNB() is used when each feature follows a Bernoulli distribution. The data or all features are binary with \n",
    "#     values 0 or 1\n",
    "#     CategoricalNB() is used when each feature has its own categorical distribution\n",
    "            \n",
    "#     (https://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               workclass    education    race   gender  income\n",
      "0              State-gov    Bachelors   White     Male   <=50K\n",
      "1       Self-emp-not-inc    Bachelors   White     Male   <=50K\n",
      "2                Private      HS-grad   White     Male   <=50K\n",
      "3                Private         11th   Black     Male   <=50K\n",
      "4                Private    Bachelors   Black   Female   <=50K\n",
      "...                  ...          ...     ...      ...     ...\n",
      "32556            Private   Assoc-acdm   White   Female   <=50K\n",
      "32557            Private      HS-grad   White     Male    >50K\n",
      "32558            Private      HS-grad   White   Female   <=50K\n",
      "32559            Private      HS-grad   White     Male   <=50K\n",
      "32560       Self-emp-inc      HS-grad   White   Female    >50K\n",
      "\n",
      "[32561 rows x 5 columns]\n",
      "            workclass      education                 race   gender  income  \\\n",
      "0           State-gov      Bachelors                White     Male   <=50K   \n",
      "1    Self-emp-not-inc      Bachelors                White     Male   <=50K   \n",
      "2             Private        HS-grad                White     Male   <=50K   \n",
      "3             Private           11th                Black     Male   <=50K   \n",
      "4             Private      Bachelors                Black   Female   <=50K   \n",
      "5             Private        Masters                White   Female   <=50K   \n",
      "6             Private            9th                Black   Female   <=50K   \n",
      "7    Self-emp-not-inc        HS-grad                White     Male    >50K   \n",
      "8             Private        Masters                White   Female    >50K   \n",
      "9             Private      Bachelors                White     Male    >50K   \n",
      "10            Private   Some-college                Black     Male    >50K   \n",
      "11          State-gov      Bachelors   Asian-Pac-Islander     Male    >50K   \n",
      "12            Private      Bachelors                White   Female   <=50K   \n",
      "13            Private     Assoc-acdm                Black     Male   <=50K   \n",
      "14            Private      Assoc-voc   Asian-Pac-Islander     Male    >50K   \n",
      "15            Private        7th-8th   Amer-Indian-Eskimo     Male   <=50K   \n",
      "16   Self-emp-not-inc        HS-grad                White     Male   <=50K   \n",
      "17            Private        HS-grad                White     Male   <=50K   \n",
      "18            Private           11th                White     Male   <=50K   \n",
      "19   Self-emp-not-inc        Masters                White   Female    >50K   \n",
      "\n",
      "    income_bin  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "5            0  \n",
      "6            0  \n",
      "7            1  \n",
      "8            1  \n",
      "9            1  \n",
      "10           1  \n",
      "11           1  \n",
      "12           0  \n",
      "13           0  \n",
      "14           1  \n",
      "15           0  \n",
      "16           0  \n",
      "17           0  \n",
      "18           0  \n",
      "19           1  \n"
     ]
    },
    {
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-69ab048e221e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Compute means for each attribute by income\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mincome_eval_cat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincome_eval_cat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'income_bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Compute standard deviations of each attribute by income status\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1230\u001b[0m         \u001b[0mnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_groupby_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"numeric_only\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m         return self._cython_agg_general(\n\u001b[1;32m-> 1232\u001b[1;33m             \u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1233\u001b[0m         )\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1002\u001b[0m     ) -> DataFrame:\n\u001b[0;32m   1003\u001b[0m         agg_blocks, agg_items = self._cython_agg_blocks(\n\u001b[1;32m-> 1004\u001b[1;33m             \u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m         )\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0magg_blocks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msplit_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No numeric types to aggregate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msplit_items\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDataError\u001b[0m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "# Question 1:\n",
    "    \n",
    "# The features in this data include workclass, education, race & gender. \n",
    "# The output variable is income & contains two categorical values (<=50k or >50k) indicating whether the income of an \n",
    "# individual is less than/equal to $50,000 or greater than $50,000 respectively. \n",
    "\n",
    "# Print the unique values of each variable in this data. \n",
    "income_eval_cat = pd.read_csv('income_evaluation_cat.csv') \n",
    "print(income_eval_cat)\n",
    "\n",
    "# Implement Naïve Bayes from scratch using Bayes’ rule - You don’t need to define a function, but you could if you want\n",
    "# You can do your calculations in Python, but you can not use the sklearn package\n",
    "lb_make = LabelEncoder()\n",
    "income_eval_cat['income_bin'] = lb_make.fit_transform(income_eval_cat[' income'])\n",
    "income_eval_cat = income_eval_cat.copy()\n",
    "print(income_eval_cat.head(20))\n",
    "\n",
    "# Compute means for each attribute by income\n",
    "\n",
    "means = income_eval_cat.groupby(income_eval_cat['income_bin']).mean()\n",
    "print(means)\n",
    "# Compute standard deviations of each attribute by income status\n",
    "sigmas = income_eval_cat.groupby(income_eval_cat['income_bin']).std()\n",
    "print(sigmas)\n",
    "\n",
    "# Extract means of attributes given y = 0, y = 1\n",
    "means_0 = means.iloc[0].values\n",
    "print(means_0)\n",
    "means_1 = means.iloc[1].values\n",
    "print(means_1)\n",
    "\n",
    "# Extract standard deviations of attributes given y = 0, y = 1\n",
    "sigmas_0 = sigmas.iloc[0].values\n",
    "print(sigmas_0)\n",
    "sigmas_1 = sigmas.iloc[1].values\n",
    "print(sigmas_1)\n",
    "\n",
    "# Suppose that all the data you uploaded is the training data, classify a \n",
    "# test instance into the class income<=50 or income>50k\n",
    "X = ['Private', 'Bachelors', 'White', 'Female'] \n",
    "print(X)\n",
    "\n",
    "# densities of attribute values given a class y0, y1\n",
    "densities_y0 = stats.norm(means_0, sigmas_0).pdf(x)\n",
    "print(densities_y0)\n",
    "densities_y1 = stats.norm(means_1, sigmas_1).pdf(x)\n",
    "print(densities_y1)\n",
    "\n",
    "# counts for each class y in the dataset\n",
    "y_counts = income_eval_cat.income.value_counts()\n",
    "print(y_counts)\n",
    "print(sum(y_counts.values))\n",
    "\n",
    "# the prior probabilities of y\n",
    "p_ys = y_counts/sum(y_counts.values)\n",
    "print(p_ys)\n",
    "p_y0 = p_ys[0]\n",
    "print(p_y0)\n",
    "p_y1 = p_ys[1]\n",
    "print(p_y1)\n",
    " \n",
    "# You need to compute the posterior probabilities P(income<=50/X) & P(income>50k/X)\n",
    "# posterior probability of class y0 given the data x p(y0/x) = p(y0)*p(x/y0)\n",
    "posterior_y0 = p_y0*np.prod(densities_y0)\n",
    "print(posterior_y0)\n",
    "# posterior probability of class y1 given the data x p(y0/x) = p(y0)*p(x/y0)\n",
    "posterior_y1 = p_y1*np.prod(densities_y1)\n",
    "print(posterior_y1)\n",
    "\n",
    "# print the class with the greater posterior probability as the predicted class maximizing a function is the same\n",
    "# as minimizing the negative of the function so include negative signs to the posteriors\n",
    "if np.argmin([-posterior_y0, -posterior_y1]) == 0:\n",
    "    print('predict class y0')\n",
    "else:\n",
    "    print('predict class_y1')\n",
    "        \n",
    "        \n",
    "# Preprocess or transform the features in the data using an appropriate scaler in sklearn \n",
    "# You don’t need to transform the output variable; it should still work fine in a text format\n",
    "# Normalize the posterior probabilities\n",
    "posterior_y1 = p_y1*np.prod(densities_y1)/sum([p_y1*np.prod(densities_y1), p_y0*np.prod(densities_y0)])\n",
    "print(posterior_y1)\n",
    "posterior_y0 = p_y0*np.prod(densities_y0)/sum([p_y1*np.prod(densities_y1), p_y0*np.prod(densities_y0)])\n",
    "print(posterior_y0)\n",
    "# Check that the posteriors add up to 1\n",
    "print(posterior_y1 + posterior_y0)\n",
    "\n",
    "# Randomly split the transformed input & output data into X_train, y_train, X_test & y_test using sklearn, use numpy \n",
    "# arrays to store the data\n",
    "X_train = income_eval_cat.iloc[:,0:3]\n",
    "X_train = np.array(X_train)\n",
    "y_train = income_eval_cat[' income'].values\n",
    "X_test = np.array(x).reshape(1, -1)\n",
    "\n",
    "# Use an appropriate Naïve Bayes constructor in sklearn to construct & fit a Naïve Bayes model on the training data\n",
    "# use the model to compute the accuracy score of the training & test set\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  education_num  hours_per_week  income\n",
      "0   39             13              40   <=50K\n",
      "1   50             13              13   <=50K\n",
      "2   38              9              40   <=50K\n",
      "3   53              7              40   <=50K\n",
      "4   28             13              40   <=50K\n",
      "    age  education_num  hours_per_week  income  income_bin\n",
      "0    39             13              40   <=50K           0\n",
      "1    50             13              13   <=50K           0\n",
      "2    38              9              40   <=50K           0\n",
      "3    53              7              40   <=50K           0\n",
      "4    28             13              40   <=50K           0\n",
      "5    37             14              40   <=50K           0\n",
      "6    49              5              16   <=50K           0\n",
      "7    52              9              45    >50K           1\n",
      "8    31             14              50    >50K           1\n",
      "9    42             13              40    >50K           1\n",
      "10   37             10              80    >50K           1\n",
      "11   30             13              40    >50K           1\n",
      "12   23             13              30   <=50K           0\n",
      "13   32             12              50   <=50K           0\n",
      "14   40             11              40    >50K           1\n",
      "15   34              4              45   <=50K           0\n",
      "16   25              9              35   <=50K           0\n",
      "17   32              9              40   <=50K           0\n",
      "18   38              7              50   <=50K           0\n",
      "19   43             14              45    >50K           1\n",
      "                  age  education_num  hours_per_week\n",
      "income_bin                                          \n",
      "0           36.783738       9.595065       38.840210\n",
      "1           44.249841      11.611657       45.473026\n",
      "                  age  education_num  hours_per_week\n",
      "income_bin                                          \n",
      "0           14.020088       2.436147       12.318995\n",
      "1           10.519028       2.385129       11.012971\n",
      "[36.78373786  9.59506472 38.84021036]\n",
      "[44.24984058 11.61165668 45.4730264 ]\n",
      "[14.02008849  2.43614679 12.31899464]\n",
      "[10.51902772  2.38512863 11.01297093]\n",
      "[0.02531168 0.16151284 0.02857872]\n",
      "[0.01515093 0.1331227  0.03619137]\n",
      "0    24720\n",
      "1     7841\n",
      "Name: income_bin, dtype: int64\n",
      "32561\n",
      "0    0.75919\n",
      "1    0.24081\n",
      "Name: income_bin, dtype: float64\n",
      "0.7591904425539756\n",
      "0.2408095574460244\n",
      "8.869959848024953e-05\n",
      "1.757802400248455e-05\n",
      "predict class y0\n",
      "0.16539722654541208\n",
      "0.8346027734545879\n",
      "1.0\n",
      "[' <=50K']\n"
     ]
    }
   ],
   "source": [
    "# Question 2 \n",
    "\n",
    "# Upload the income_evaluation_continuous.csv data provided on canvas\n",
    "inc_eval_cont = pd.read_csv('income_evaluation_continuous.csv')\n",
    "print(inc_eval_cont.head())\n",
    "# The features in this data include age, education_num, & hours_per_week\n",
    "# The output variable is income & contains two categorical values (<=50k or >50k) indicating whether the income of an\n",
    "# individual is less than/equal to $50,000 or greater than $50,000 respectively\n",
    "\n",
    "# Implement Naïve Bayes from scratch using Bayes’ rule - You don’t need to define a function, but you could if you want\n",
    "# You can do your calculations in Python, but you can not use the sklearn package\n",
    "# Assume that all the features or input variables follow a normal distribution\n",
    "lb_make = LabelEncoder()\n",
    "inc_eval_cont['income_bin'] = lb_make.fit_transform(inc_eval_cont[' income'])\n",
    "inc_eval_cont = inc_eval_cont.copy()\n",
    "print(inc_eval_cont.head(20))\n",
    "\n",
    "# Compute the mean & standard deviation of each input variable such that the results are presented on the same table \n",
    "# or data frame. You can call the .apply() function on the pandas DataFrame.  \n",
    "\n",
    "# Compute means for each attribute by income\n",
    "means = inc_eval_cont.groupby(inc_eval_cont['income_bin']).mean()\n",
    "print(means)\n",
    "# Compute standard deviations of each attribute by income status\n",
    "sigmas = inc_eval_cont.groupby(inc_eval_cont['income_bin']).std()\n",
    "print(sigmas)\n",
    "# Extract means of attributes given y = 0, y = 1\n",
    "means_0 = means.iloc[0].values\n",
    "print(means_0)\n",
    "means_1 = means.iloc[1].values\n",
    "print(means_1)\n",
    "# Extract standard deviations of attributes given y = 0, y = 1\n",
    "sigmas_0 = sigmas.iloc[0].values\n",
    "print(sigmas_0)\n",
    "sigmas_1 = sigmas.iloc[1].values\n",
    "print(sigmas_1)\n",
    "\n",
    "# Given that all the income_evaluation_continuous.csv data you uploaded is the training data , classify a test instance,\n",
    "# into the class income<=50 or income>50k. \n",
    "X = [30, 10, 45]\n",
    "\n",
    "# densities of attribute values given a class y0, y1\n",
    "densities_y0 = stats.norm(means_0, sigmas_0).pdf(X)\n",
    "print(densities_y0)\n",
    "densities_y1 = stats.norm(means_1, sigmas_1).pdf(X)\n",
    "print(densities_y1)\n",
    "\n",
    "# counts for each class y in the dataset\n",
    "y_counts = inc_eval_cont.income_bin.value_counts()\n",
    "print(y_counts)\n",
    "print(sum(y_counts.values))\n",
    "\n",
    "# the prior probabilities of y\n",
    "p_ys = y_counts/sum(y_counts.values)\n",
    "print(p_ys)\n",
    "p_y0 = p_ys[0]\n",
    "print(p_y0)\n",
    "p_y1 = p_ys[1]\n",
    "print(p_y1)\n",
    " \n",
    "# Compute the posterior probabilities P(income<=50/X) & P(income>50k/X)\n",
    "# posterior probability of class y0 given the data x p(y0/x) = p(y0)*p(x/y0)\n",
    "posterior_y0 = p_y0*np.prod(densities_y0)\n",
    "print(posterior_y0)\n",
    "# posterior probability of class y1 given the data x p(y0/x) = p(y0)*p(x/y0)\n",
    "posterior_y1 = p_y1*np.prod(densities_y1)\n",
    "print(posterior_y1)\n",
    "\n",
    "# Print the class with the greater posterior probability as the predicted class. \n",
    "if np.argmin([-posterior_y0, -posterior_y1]) == 0:\n",
    "    print('predict class y0')\n",
    "else:\n",
    "    print('predict class_y1')\n",
    "        \n",
    "# Preprocess or transform the features in the income_evaluation_cont.csv data using an appropriate scaler in sklearn\n",
    "# You don’t need to transform the output variable; it should still work fine in a text format \n",
    "# Normalize the posterior probabilities\n",
    "posterior_y1 = p_y1*np.prod(densities_y1)/sum([p_y1*np.prod(densities_y1), p_y0*np.prod(densities_y0)])\n",
    "print(posterior_y1)\n",
    "posterior_y0 = p_y0*np.prod(densities_y0)/sum([p_y1*np.prod(densities_y1), p_y0*np.prod(densities_y0)])\n",
    "print(posterior_y0)\n",
    "# Check that the posteriors add up to 1\n",
    "print(posterior_y1 + posterior_y0)\n",
    "\n",
    "# Randomly split the input & output data into X_train, y_train, X_test and y_test using tools in sklearn \n",
    "X_train = inc_eval_cont.iloc[:,0:3]\n",
    "X_train = np.array(X_train)\n",
    "y_train = inc_eval_cont[' income'].values\n",
    "X_test = np.array(X).reshape(1, -1)\n",
    "\n",
    "# Use an appropriate Naïve Bayes constructor in sklearn to construct & fit a Naïve Bayes model on the training data\n",
    "# Use the model to compute the accuracy score of the training and test set. \n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3  FBI Russia probe helped by Australian diplomat...   \n",
      "4  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text news_type  \n",
      "0  WASHINGTON (Reuters) - The head of a conservat...      True  \n",
      "1  WASHINGTON (Reuters) - Transgender people will...      True  \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...      True  \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...      True  \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...      True  \n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text news_type  \n",
      "0  Donald Trump just couldn t wish all Americans ...     False  \n",
      "1  House Intelligence Committee Chairman Devin Nu...     False  \n",
      "2  On Friday, it was revealed that former Milwauk...     False  \n",
      "3  On Christmas day, Donald Trump announced that ...     False  \n",
      "4  Pope Francis used his annual Christmas Day mes...     False  \n",
      "                                                   title  \\\n",
      "0      As U.S. budget fight looms, Republicans flip t...   \n",
      "1      U.S. military to accept transgender recruits o...   \n",
      "2      Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3      FBI Russia probe helped by Australian diplomat...   \n",
      "4      Trump wants Postal Service to charge 'much mor...   \n",
      "...                                                  ...   \n",
      "23475  Hillary Clinton: ‘Israel First’ (and no peace ...   \n",
      "23476  McPain: John McCain Furious That Iran Treated ...   \n",
      "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
      "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
      "23479  How to Blow $700 Million: Al Jazeera America F...   \n",
      "\n",
      "                                                    text news_type  \n",
      "0      WASHINGTON (Reuters) - The head of a conservat...      True  \n",
      "1      WASHINGTON (Reuters) - Transgender people will...      True  \n",
      "2      WASHINGTON (Reuters) - The special counsel inv...      True  \n",
      "3      WASHINGTON (Reuters) - Trump campaign adviser ...      True  \n",
      "4      SEATTLE/WASHINGTON (Reuters) - President Donal...      True  \n",
      "...                                                  ...       ...  \n",
      "23475  Robert Fantina CounterpunchAlthough the United...     False  \n",
      "23476  21st Century Wire says As 21WIRE reported earl...     False  \n",
      "23477  21st Century Wire says It s a familiar theme. ...     False  \n",
      "23478  Patrick Henningsen  21st Century WireRemember ...     False  \n",
      "23479  21st Century Wire says Al Jazeera America will...     False  \n",
      "\n",
      "[44897 rows x 3 columns]\n",
      "                                                   title  \\\n",
      "0      As U.S. budget fight looms, Republicans flip t...   \n",
      "1      U.S. military to accept transgender recruits o...   \n",
      "2      Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3      FBI Russia probe helped by Australian diplomat...   \n",
      "4      Trump wants Postal Service to charge 'much mor...   \n",
      "...                                                  ...   \n",
      "23475  Hillary Clinton: ‘Israel First’ (and no peace ...   \n",
      "23476  McPain: John McCain Furious That Iran Treated ...   \n",
      "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
      "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
      "23479  How to Blow $700 Million: Al Jazeera America F...   \n",
      "\n",
      "                                                    text news_type  \\\n",
      "0      WASHINGTON (Reuters) - The head of a conservat...      True   \n",
      "1      WASHINGTON (Reuters) - Transgender people will...      True   \n",
      "2      WASHINGTON (Reuters) - The special counsel inv...      True   \n",
      "3      WASHINGTON (Reuters) - Trump campaign adviser ...      True   \n",
      "4      SEATTLE/WASHINGTON (Reuters) - President Donal...      True   \n",
      "...                                                  ...       ...   \n",
      "23475  Robert Fantina CounterpunchAlthough the United...     False   \n",
      "23476  21st Century Wire says As 21WIRE reported earl...     False   \n",
      "23477  21st Century Wire says It s a familiar theme. ...     False   \n",
      "23478  Patrick Henningsen  21st Century WireRemember ...     False   \n",
      "23479  21st Century Wire says Al Jazeera America will...     False   \n",
      "\n",
      "                                                    news  \n",
      "0      As U.S. budget fight looms, Republicans flip t...  \n",
      "1      U.S. military to accept transgender recruits o...  \n",
      "2      Senior U.S. Republican senator: 'Let Mr. Muell...  \n",
      "3      FBI Russia probe helped by Australian diplomat...  \n",
      "4      Trump wants Postal Service to charge 'much mor...  \n",
      "...                                                  ...  \n",
      "23475  Hillary Clinton: ‘Israel First’ (and no peace ...  \n",
      "23476  McPain: John McCain Furious That Iran Treated ...  \n",
      "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...  \n",
      "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...  \n",
      "23479  How to Blow $700 Million: Al Jazeera America F...  \n",
      "\n",
      "[44897 rows x 4 columns]\n",
      "  news_type                                               news\n",
      "0      True  As U.S. budget fight looms, Republicans flip t...\n",
      "1      True  U.S. military to accept transgender recruits o...\n",
      "2      True  Senior U.S. Republican senator: 'Let Mr. Muell...\n",
      "3      True  FBI Russia probe helped by Australian diplomat...\n",
      "4      True  Trump wants Postal Service to charge 'much mor...\n"
     ]
    }
   ],
   "source": [
    "# Question 3:\n",
    "# Implement a Naïve Bayes for text classification to detect fake or true news\n",
    "\n",
    "# Read in given data\n",
    "true = pd.read_csv('True.csv')\n",
    "trueDF = pd.DataFrame(true)\n",
    "# print(trueDF.head())\n",
    "\n",
    "# Create a new data frame by selecting the “title” & “text” columns, then add a new column called “news_type” where all the \n",
    "# values on this new column are “True” - So, your new data frame should have three columns; “title”, “text” & “news_type” \n",
    "t_DF = trueDF[['title', 'text']].copy()\n",
    "t_DF['news_type'] = 'True'\n",
    "print(t_DF.head())\n",
    "\n",
    "# Read in given data\n",
    "fake = pd.read_csv('Fake.csv')\n",
    "fakeDF = pd.DataFrame(fake)\n",
    "# print(fakeDF.head())\n",
    "\n",
    "# Create a new data frame by selecting the “title” & “text” columns, then add a new column called “news_type” where all the\n",
    "# values on this new column are “Fake” - So, your new data frame should have three columns; “title”, “text” & “news_type”\n",
    "f_DF = fakeDF[['title', 'text']].copy()\n",
    "f_DF['news_type'] = 'False'\n",
    "print(f_DF.head())\n",
    "\n",
    "# Merge the data frame in a) & b) so that one of the data frames is stacked vertically on top of the other. \n",
    "m_DF = t_DF.append(f_DF)\n",
    "print(m_DF.head(-1))\n",
    "\n",
    "# Combine the text in the “title” & “text” columns of the merged data frame into another column called “news”\n",
    "m_DF['news'] = m_DF['title'] + ' ' + m_DF['text']\n",
    "print(m_DF.head(-1))\n",
    "\n",
    "# Drop the “title” & “text” columns so that your final data frame is has only two columns, “news” & “news_type”\n",
    "final = m_DF[['news_type', 'news']].copy()\n",
    "\n",
    "# Print the first five rows of your final data frame\n",
    "print(final.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{']', '\\\\', '@', ',', '(', '.', '~', '=', '!', '$', ')', '[', '/', '?', ';', '}', '+', '|', '%', \"'\", '-', '&', '>', '#', '{', '*', '\"', '^', '`', '<', '_', ':'}\n",
      "501    Tillerson 'offended' by claims of State Depart...\n",
      "502    Trump to make remarks at White House at 3 p.m....\n",
      "503    U.S. budget chief Mulvaney says CFPB staff sho...\n",
      "504    Russian envoy to U.S. to inspect San Francisco...\n",
      "505    White House to Democratic leaders: 'stop the p...\n",
      "506    Top Democrats in Congress say won't meet with ...\n",
      "507    U.S. Senate liberals propose new steps for Pue...\n",
      "508    Trump: 'I don't see a deal' with Democrats on ...\n",
      "509    Trump-installed consumer agency head sets hiri...\n",
      "Name: news, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_type</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>u.s budget fight looms republicans flip fiscal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>u.s military accept transgender recruits monda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>senior u.s republican senator let mr mueller j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>fbi russia probe helped australian diplomat ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>trump wants postal service charge much amazon ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  news_type                                               news\n",
       "0      True  u.s budget fight looms republicans flip fiscal...\n",
       "1      True  u.s military accept transgender recruits monda...\n",
       "2      True  senior u.s republican senator let mr mueller j...\n",
       "3      True  fbi russia probe helped australian diplomat ti...\n",
       "4      True  trump wants postal service charge much amazon ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess your data by cleaning the textual data in the “news” column & removing the stop words, special characters, \n",
    "# punctuations, etc especially at the beginning & end of each word\n",
    "\n",
    "# initialize stopwords\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "list(sw)[0:10] # view the first 10 stopwords\n",
    "\n",
    "# view puntuations and special characters that need to be removed\n",
    "print(set(string.punctuation))\n",
    "\n",
    "# function that cleans text and removes stop words\n",
    "def clean(text, stopwords):\n",
    "    # remove tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # split text on whitespace\n",
    "    text_list = text.split()\n",
    "    text_words = []\n",
    "    \n",
    "    punctuation = set(string.punctuation)\n",
    "     # keep #tags and @mentions\n",
    "     ## punctuation.remove(\"#\")\n",
    "     ## punctuation.remove(\"@\")\n",
    "    \n",
    "    for word in text_list:\n",
    "     # remove punctuation marks at the beginning\n",
    "     # of each word\n",
    "        while len(word) > 0 and word[0] in punctuation:\n",
    "            word = word[1:]\n",
    "\n",
    "        # remove punctuation marks at the end of each word\n",
    "        while len(word) > 0 and word[-1] in punctuation:\n",
    "            word = word[:-1]\n",
    "\n",
    "        # a rule to eliminate most urls\n",
    "        if len(word) > 0 and \"/\" not in word:\n",
    "            # eliminate stopwords\n",
    "            if word.lower() not in stopwords:\n",
    "                # append the word to the text_words list\n",
    "                text_words.append(word.lower())\n",
    "        cleaner_text = \" \".join(text_words)\n",
    "    return cleaner_text\n",
    "\n",
    "# display a few lines of messy news text before cleaning\n",
    "print(final[\"news\"][501:510])\n",
    "\n",
    "# Apply the clean() function to the data & pass in the stopword argument\n",
    "final[\"news\"] = final[\"news\"].apply(clean, stopwords=sw)\n",
    "final.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501    tillerson offended claims state department's h...\n",
      "502    trump make remarks white house 3 p.m est washi...\n",
      "503    u.s budget chief mulvaney says cfpb staff disr...\n",
      "504    russian envoy u.s inspect san francisco consul...\n",
      "505    white house democratic leaders stop political ...\n",
      "506    top democrats congress say meet trump planned ...\n",
      "507    u.s senate liberals propose new steps puerto r...\n",
      "508    trump see deal democrats keeping government op...\n",
      "509    trump-installed consumer agency head sets hiri...\n",
      "Name: news, dtype: object\n",
      "(44898, 2)\n",
      "(44828, 2)\n",
      "  news_type                                               news\n",
      "0      True  u.s budget fight looms republicans flip fiscal...\n",
      "1      True  u.s military accept transgender recruits monda...\n",
      "2      True  senior u.s republican senator let mr mueller j...\n",
      "3      True  fbi russia probe helped australian diplomat ti...\n",
      "4      True  trump wants postal service charge much amazon ...\n",
      "(31379,) (31379,)\n",
      "(13449,) (13449,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31379, 88027)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display messy news text after cleaning\n",
    "print(final[\"news\"][501:510])\n",
    "\n",
    "# check shape of the data\n",
    "print(final.shape)\n",
    "\n",
    "# Drop instances where the news text is less than 50 words for training \n",
    "final = final[final[\"news\"].str.len() > 50]\n",
    "print(final.shape)\n",
    "print(final.head())\n",
    "\n",
    "# Spit the feature vectors & the output variable into into X_train, y_train, X_test & y_test \n",
    "# Let the test set be 30% of the entire data\n",
    "X_train, X_test, y_train, y_test = train_test_split(final[\"news\"], final[\"news_type\"], test_size= 0.3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# Transform the input text data into feature vectors where the entries of the feature vectors are \n",
    "# term-frequency-inverse-document-frequency - Use the TfidfVectorizer() in sklearn\n",
    "tfidf = TfidfVectorizer(ngram_range= (1,2), stop_words= \"english\", min_df= 10, max_features= None)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_train_tfidf.toarray()\n",
    "\n",
    "X_train_tfidf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['False' 'True' 'False' ... 'True' 'False' 'False']\n",
      "0.9577105707638867\n",
      "0.953305078444494\n"
     ]
    }
   ],
   "source": [
    "# Fit an appropriate Naïve Bayes model & compute the training & test accuracy of the model. \n",
    "clf = MultinomialNB()\n",
    "clf = clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# make prediction on training set\n",
    "print(clf.predict(X_train_tfidf))\n",
    "\n",
    "# compute accuracy on training set\n",
    "print(clf.score(X_train_tfidf, y_train))\n",
    "\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "X_test_tfidf.toarray()\n",
    "\n",
    "print(clf.score(X_test_tfidf, y_test))\n",
    "\n",
    "# Q: Is there overfitting?\n",
    "# A: No, there does not seem to be because the training and test accuracy scores are similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross validation score:  0.9493930308085694\n",
      "Standard deviation of cross validation scores:  0.0025839205649281645\n"
     ]
    }
   ],
   "source": [
    "# Fit a Naïve Bayes using cross validation & print the average cross validation score as well as the standard deviation \n",
    "# of the cross-validation scores\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range= (1,2), stop_words= \"english\", min_df= 10, max_features= None)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "scores = cross_val_score(estimator= MultinomialNB(), X= X_train_tfidf, y= y_train, cv= 5)\n",
    "print(\"Average cross validation score: \", scores.mean())\n",
    "print(\"Standard deviation of cross validation scores: \", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 10,\n",
       " 'ngram_range': (1, 2),\n",
       " 'norm': 'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': 'english',\n",
       " 'strip_accents': None,\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select some hypermeters of your choice & tune using the grid search cross validation. \n",
    "# Use some other hyperparameters than those used in class examples\n",
    "tfidf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([(\"tfidf\",TfidfVectorizer(stop_words= \"english\")), (\"nb\", MultinomialNB())])\n",
    "# param_grid = [{\"tfidf__min_df\":[5, 20], \"tfidf__ngram_range\":[(1, 1), (1, 2), (1, 3), (1, 5), (1, 7)]}]\n",
    "# grid = GridSearchCV(estimator= pipe , param_grid= param_grid, cv= 5)\n",
    "# grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find best hyperparameter values\n",
    "# grid.best_params_\n",
    "# # training accuracy\n",
    "# grid.score(X_train, y_train)\n",
    "# # test accuracy\n",
    "# grid.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
