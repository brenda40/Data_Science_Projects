{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMP 4448: Data Science Tools II\n",
    "Assignment 1\n",
    "Brenda Woodard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.preprocessing \n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler, LabelBinarizer)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanTemp  TotPrecip TotSnow  Max24hrPrecip Max24hrSnow\n",
      "0      21.0       2.72    24.8           0.74         6.7\n",
      "1      27.6       4.11    32.5           1.01        12.2\n",
      "2      39.3       3.72      11           1.15           6\n",
      "3      45.1       5.68       2           1.19           2\n",
      "4      54.6       5.26       0           3.23           0\n",
      "   Month  Year  LowTemp  HighTemp  WarmestMin  ColdestHigh  AveMin  AveMax  \\\n",
      "0      1  1920     -8.0      48.0        37.0         12.0    12.5    29.5   \n",
      "1      2  1920     -7.0      52.0        34.0         16.0    20.6    34.6   \n",
      "2      3  1920     10.0      72.0        45.0         25.0    30.5    48.0   \n",
      "3      4  1920     26.0      67.0        46.0         37.0    38.0    52.1   \n",
      "4      5  1920     36.0      86.0        61.0         45.0    47.5    61.8   \n",
      "\n",
      "   meanTemp  TotPrecip TotSnow  Max24hrPrecip Max24hrSnow  \n",
      "0      21.0       2.72    24.8           0.74         6.7  \n",
      "1      27.6       4.11    32.5           1.01        12.2  \n",
      "2      39.3       3.72      11           1.15           6  \n",
      "3      45.1       5.68       2           1.19           2  \n",
      "4      54.6       5.26       0           3.23           0  \n",
      "     Month  Year  LowTemp  HighTemp  WarmestMin  ColdestHigh  AveMin  AveMax  \\\n",
      "235      8  2014     58.0      91.0        70.0         69.0    63.2    78.4   \n",
      "236      9  2014     46.0      93.0        72.0         56.0    58.3    74.2   \n",
      "237     10  2014     39.0      81.0        61.0         52.0    49.7    63.0   \n",
      "238     11  2014     23.0      66.0        48.0         34.0    35.5    49.5   \n",
      "239     12  2014     18.0      64.0        44.0         29.0    32.8    43.7   \n",
      "\n",
      "     meanTemp  TotPrecip TotSnow  Max24hrPrecip Max24hrSnow  \n",
      "235      70.8       1.75       0           1.06           0  \n",
      "236      66.2       0.70       0           0.22           0  \n",
      "237      56.4       5.83       0           2.25           0  \n",
      "238      42.5       5.27     2.6           1.64         2.3  \n",
      "239      38.3       6.56     0.3           2.90         0.3  \n",
      "(1140, 13)\n",
      "Month            0\n",
      "Year             0\n",
      "LowTemp          0\n",
      "HighTemp         0\n",
      "WarmestMin       0\n",
      "ColdestHigh      0\n",
      "AveMin           0\n",
      "AveMax           0\n",
      "meanTemp         0\n",
      "TotPrecip        0\n",
      "TotSnow          0\n",
      "Max24hrPrecip    0\n",
      "Max24hrSnow      0\n",
      "dtype: int64\n",
      "(1112, 13)\n",
      "Index(['month', 'year', 'low_temp', 'high_temp', 'warmest_min', 'coldest_high',\n",
      "       'ave_min', 'ave_max', 'mean_temp', 'tot_precip', 'tot_snow',\n",
      "       'max_24hr_precip', 'max_24hr_snow'],\n",
      "      dtype='object')\n",
      "[1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1931 1932 1933 1934\n",
      " 1935 1936 1937 1938 1939 1940]\n",
      "   month  year  low_temp  high_temp  warmest_min  coldest_high  ave_min  \\\n",
      "5      6  1995      53.0       95.0         73.0          61.0     60.2   \n",
      "6      7  1995      59.0      100.0         75.0          67.0     67.1   \n",
      "7      8  1995      55.0       96.0         72.0          66.0     64.1   \n",
      "\n",
      "   ave_max  mean_temp  tot_precip tot_snow  max_24hr_precip max_24hr_snow  \n",
      "5     77.0       68.6        1.55        0             0.53             0  \n",
      "6     84.6       75.9        2.06        0             0.88             0  \n",
      "7     81.5       72.8        0.82        0             0.63             0  \n",
      "    month  year  low_temp  high_temp  warmest_min  coldest_high  ave_min  \\\n",
      "6       7  1920      56.0       90.0         71.0          65.0     63.4   \n",
      "7       8  1920      56.0       93.0         74.0          69.0     65.2   \n",
      "16      5  1921      41.0       93.0         70.0          48.0     50.0   \n",
      "19      8  1921      55.0       92.0         72.0          69.0     61.5   \n",
      "20      9  1921      53.0       93.0         75.0          64.0     60.1   \n",
      "29      6  1922      52.0       92.0         72.0          59.0     60.8   \n",
      "30      7  1922      58.0       93.0         72.0          64.0     64.4   \n",
      "31      8  1922      54.0       93.0         72.0          65.0     63.4   \n",
      "41      6  1923      47.0       96.0         74.0          53.0     59.6   \n",
      "42      7  1923      56.0       94.0         72.0          62.0     62.2   \n",
      "43      8  1923      51.0       94.0         74.0          63.0     60.5   \n",
      "53      6  1924      48.0       90.0         68.0          58.0     58.2   \n",
      "54      7  1924      58.0       95.0         73.0          68.0     64.8   \n",
      "55      8  1924      56.0       98.0         75.0          68.0     64.4   \n",
      "56      9  1924      44.0       90.0         71.0          59.0     54.4   \n",
      "65      6  1925      53.0      100.0         80.0          59.0     61.1   \n",
      "66      7  1925      54.0       95.0         73.0          66.0     64.1   \n",
      "67      8  1925      50.0       92.0         70.0          67.0     62.3   \n",
      "68      9  1925      41.0       90.0         72.0          56.0     56.0   \n",
      "78      7  1926      56.0      103.0         75.0          64.0     62.6   \n",
      "\n",
      "    ave_max  mean_temp  tot_precip tot_snow  max_24hr_precip max_24hr_snow  \n",
      "6      81.4       72.4        1.56        0             0.76             0  \n",
      "7      78.9       72.0        2.32        0             1.00             0  \n",
      "16     66.1       58.0        3.63        0             1.82             0  \n",
      "19     77.7       69.6        1.63        0             1.12             0  \n",
      "20     76.9       68.5        1.22        0             0.36             0  \n",
      "29     76.5       68.7        8.05        0             3.32             0  \n",
      "30     79.7       72.0        2.63        0             0.86             0  \n",
      "31     77.4       70.4        4.75        0             1.23             0  \n",
      "41     79.0       69.3        2.03        0             1.02             0  \n",
      "42     77.7       69.9        3.36        0             0.78             0  \n",
      "43     78.1       69.3        2.07        0             0.78             0  \n",
      "53     75.1       66.7        1.07        0             0.36             0  \n",
      "54     82.9       73.8        2.04        0             0.70             0  \n",
      "55     79.5       71.9        6.86        0             4.15             0  \n",
      "56     70.0       62.2        6.96        0             3.89             0  \n",
      "65     80.7       70.9        4.59        0             1.36             0  \n",
      "66     80.5       72.3        3.54        0             1.19             0  \n",
      "67     80.0       71.1        1.40        0             0.47             0  \n",
      "68     72.1       64.1        3.45        0             1.46             0  \n",
      "78     78.4       70.4        6.06        0             2.33             0  \n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "\n",
    "# Read the boston dataset csv files into Python\n",
    "bw1 = pd.read_csv('boston_weather_1.csv')\n",
    "bw2 = pd.read_csv('boston_weather_2.csv')\n",
    "bw3 = pd.read_csv('boston_weather_3.csv')\n",
    "bw4 = pd.read_csv('boston_weather_4.csv')\n",
    "w_var = pd.read_csv('more_weather_variables.csv')\n",
    "\n",
    "# Assign the datasets to DataFrames\n",
    "boston_1 = pd.DataFrame(bw1)\n",
    "boston_2 = pd.DataFrame(bw2)\n",
    "boston_3 = pd.DataFrame(bw3)\n",
    "boston_4 = pd.DataFrame(bw4)\n",
    "more_variables = pd.DataFrame(w_var)\n",
    "print(more_variables.head())\n",
    "\n",
    "# Combine or concatenate the DataFrames, boston_1, boston_2, boston_3, and boston_4 and assign the results to a DataFrame \n",
    "# The four datasets should be combined vertically since they have the same variable names.\n",
    "merge1_2 = pd.concat([boston_1, boston_2])\n",
    "merge2_3 = pd.concat([merge1_2, boston_3])\n",
    "combined_boston = pd.concat([merge2_3, boston_4])\n",
    "\n",
    "# Horizontally merge, join or concatenate the combined_boston and more_variables DataFrames \n",
    "boston_data = pd.concat([combined_boston, more_variables.set_index(combined_boston.index)], axis = 1)\n",
    "\n",
    "# Print the first five rows of the boston_data, and the last five rows of the boston_data. \n",
    "print(boston_data.head(5))\n",
    "print(boston_data.tail(5))\n",
    "\n",
    "# print out the shape of the boston_data. \n",
    "print(boston_data.shape)\n",
    "\n",
    "# Check the boston_data to verify how many missing data points exist under each column. \n",
    "boston_data.isnull().sum()\n",
    "\n",
    "# Drop the rows or instances that contain any missing data and assign the resulting DataFrame to a variable called \n",
    "clean_boston_data = boston_data.dropna(axis = 0)\n",
    "\n",
    "# Check for missing data again to ensure there is no missing data in the clean_boston_data.\n",
    "print(clean_boston_data.isnull().sum())\n",
    "\n",
    "# Print the shape of the clean_boston_data\n",
    "print(clean_boston_data.shape)\n",
    "\n",
    "# Format all the column names to lowercase and include underscore between column names that consist of two words. \n",
    "\n",
    "# Reassign the DataFrame with the formatted column names to the same variable\n",
    "clean_boston_data = clean_boston_data.rename(columns = {'meanTemp':'mean_temp', \n",
    "                                                       'TotPrecip':'tot_precip',\n",
    "                                                       'TotSnow':'tot_snow',\n",
    "                                                       'Max24hrPrecip':'max_24hr_precip',\n",
    "                                                       'Max24hrSnow':'max_24hr_snow',\n",
    "                                                       'Month':'month', \n",
    "                                                       'Year':'year',\n",
    "                                                       'LowTemp':'low_temp',\n",
    "                                                       'HighTemp':'high_temp',\n",
    "                                                       'WarmestMin':'warmest_min',\n",
    "                                                       'ColdestHigh':'coldest_high',\n",
    "                                                       'AveMin':'ave_min',\n",
    "                                                       'AveMax':'ave_max'})\n",
    "\n",
    "# Print or output the columns of the clean_boston_data DataFrame. \n",
    "print(clean_boston_data.columns)\n",
    "\n",
    "# Select or slice all data from the clean_boston_data DataFrame, except the data where the Year is 1930. \n",
    "excluding_1930 = clean_boston_data[~(clean_boston_data.year==1930)]\n",
    "\n",
    "# Using the excluding_1930 DataFrame, output the first 20 unique values in the Year column.\n",
    "print(excluding_1930['year'].unique()[:20])\n",
    "\n",
    "# Select the data from the clean_boston_data where the Year is 1995 AND the high_temp is greater than or equal to 90.\n",
    "# Output or display the whole selected data.  \n",
    "print(clean_boston_data[(clean_boston_data.year==1995)&(clean_boston_data.high_temp>=90)].dropna())\n",
    "\n",
    "# Select the data from the clean_boston_data where the Year is 1995 OR the high_temp is greater than 89. \n",
    "# Output or print the first 20 rows of the selected data. \n",
    "x = clean_boston_data[(clean_boston_data.year==1995)|(clean_boston_data.high_temp>89)].dropna()\n",
    "print(x.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vanilla' 'chocollate ' 'vanilla' 'strawberry ' 'strawberry '\n",
      " 'chocollate ' 'vanilla' 'vanilla' 'chocollate ' 'chocollate ' 'vanilla'\n",
      " 'strawberry ' 'strawberry ' 'chocollate ' 'vanilla' 'vanilla']\n",
      "[[ 90  82  90]\n",
      " [100  73  92]\n",
      " [ 80  92  95]\n",
      " [ 95 100  82]\n",
      " [ 90  79  81]\n",
      " [ 90  90  90]\n",
      " [100  89  91]\n",
      " [ 80  85  79]\n",
      " [ 85  99  85]\n",
      " [100  73  92]\n",
      " [ 80  92  95]\n",
      " [ 95 100  82]\n",
      " [ 90  79  81]\n",
      " [ 86  92  90]\n",
      " [100  89  89]\n",
      " [ 83  95  79]]\n",
      "[array([False,  True, False, False, False,  True, False, False,  True,\n",
      "        True, False, False, False,  True, False, False])]\n",
      "Chocolate Only [[100  73  92]\n",
      " [ 90  90  90]\n",
      " [ 85  99  85]\n",
      " [100  73  92]\n",
      " [ 86  92  90]]\n",
      "Chocolate or Vanilla [[ 90  82  90]\n",
      " [100  73  92]\n",
      " [ 80  92  95]\n",
      " [ 90  90  90]\n",
      " [100  89  91]\n",
      " [ 80  85  79]\n",
      " [ 85  99  85]\n",
      " [100  73  92]\n",
      " [ 80  92  95]\n",
      " [ 86  92  90]\n",
      " [100  89  89]\n",
      " [ 83  95  79]]\n",
      "Everything But Chocolate [[ 95 100  82]\n",
      " [ 90  79  81]\n",
      " [ 95 100  82]\n",
      " [ 90  79  81]]\n",
      "Math & Reading Scores for Chocolate Only [[100  73]\n",
      " [ 90  90]\n",
      " [ 85  99]\n",
      " [100  73]\n",
      " [ 86  92]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([92.2, 85.4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2 \n",
    "\n",
    "# Read the student_data file provided into Python\n",
    "# The data shows the different midterm scores of students in math, reading & science, & their favorite ice cream flavors. \n",
    "data = pd.read_excel('student_data.xlsx')\n",
    "\n",
    "# Drop the first empty column in Python and assign the DataFrame to a variable student_data.  \n",
    "student_data = data.drop('Unnamed: 0', axis = 'columns')\n",
    "student_data = student_data.rename(columns = {'science ':'science'})\n",
    "\n",
    "# Select the data in the ice_cream_flavor column, convert the flavors to a numpy array, assign it to a var called flavor\n",
    "flavor = np.array(student_data.ice_cream_flavor)\n",
    "\n",
    "# From the student_data, select math, reading & science scores all at once, convert to a numpy array, assign to var scores \n",
    "scores = np.array(student_data[['math','reading','science']])\n",
    "\n",
    "# Print the data in the flavor and scores arrays.\n",
    "print(flavor)\n",
    "print(scores)\n",
    "\n",
    "# Use the scores & flavor arrays to slice out the scores where the flavor is chocolate only. \n",
    "f = [flavor=='chocollate ']\n",
    "print(f)\n",
    "\n",
    "s = scores[[1,5,8,9,13]]\n",
    "print('Chocolate Only', s)\n",
    "\n",
    "# Use the scores and flavor arrays to slice out the scores where the flavor is chocolate OR vanilla. \n",
    "s2 = scores[[0,1,2,5,6,7,8,9,10,13,14,15]]\n",
    "print('Chocolate or Vanilla', s2)\n",
    "\n",
    "# Use the scores and flavor arrays to slice out the scores where the flavor is not chocolate (you can use the “~ “sign). \n",
    "s3 = scores[[3,4,11,12]]\n",
    "print('Everything But Chocolate', s3)\n",
    "\n",
    "# Slice out all math and reading scores where the flavor is chocolate, then compute the mean of math and reading scores for\n",
    "# this subset (you can use the .mean() method on the array and specify the axis parameter value appropriately). \n",
    "scores2 = np.array(student_data[['math','reading']])\n",
    "s4 = scores2[[1,5,8,9,13]]\n",
    "print('Math & Reading Scores for Chocolate Only', s4)\n",
    "\n",
    "s4.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    math  reading  science Dummy Vars\n",
      "0     90       82       90  [0, 0, 1]\n",
      "1    100       73       92  [1, 0, 0]\n",
      "2     80       92       95  [0, 0, 1]\n",
      "3     95      100       82  [0, 1, 0]\n",
      "4     90       79       81  [0, 1, 0]\n",
      "5     90       90       90  [1, 0, 0]\n",
      "6    100       89       91  [0, 0, 1]\n",
      "7     80       85       79  [0, 0, 1]\n",
      "8     85       99       85  [1, 0, 0]\n",
      "9    100       73       92  [1, 0, 0]\n",
      "10    80       92       95  [0, 0, 1]\n",
      "11    95      100       82  [0, 1, 0]\n",
      "12    90       79       81  [0, 1, 0]\n",
      "13    86       92       90  [1, 0, 0]\n",
      "14   100       89       89  [0, 0, 1]\n",
      "15    83       95       79  [0, 0, 1]\n",
      "    math  reading  science\n",
      "0     90       82       90\n",
      "1    100       73       92\n",
      "2     80       92       95\n",
      "3     95      100       82\n",
      "4     90       79       81\n",
      "5     90       90       90\n",
      "6    100       89       91\n",
      "7     80       85       79\n",
      "8     85       99       85\n",
      "9    100       73       92\n",
      "10    80       92       95\n",
      "11    95      100       82\n",
      "12    90       79       81\n",
      "13    86       92       90\n",
      "14   100       89       89\n",
      "15    83       95       79\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "\n",
    "# Imagine that you wanted to use the student_data in question 2a to make predictions such that the ice_cream_flavor, math & \n",
    "# reading columns are input variables and science column is the output variable you want to predict. \n",
    "\n",
    "# Use the LabelBinarizer() to transform the ice_cream_flavor column in the student_data to dummy variables\n",
    "x = student_data.ice_cream_flavor\n",
    "label_binarizer = LabelBinarizer()\n",
    "dummy_vars = label_binarizer.fit(x).transform(x)\n",
    "\n",
    "# join these dummy variables to the student_data & drop the original ice_cream_flavor column. Reassign the resulting \n",
    "# DataFrame to a variable called student_data_1. \n",
    "student_data_1 = student_data.drop('ice_cream_flavor', axis = 'columns')\n",
    "student_data_1['Dummy Vars'] = [i for i in dummy_vars]\n",
    "\n",
    "# Print out the entire dataset\n",
    "print(student_data_1)\n",
    "\n",
    "# Extract the math, reading and science scores & use the StandardScaler() class in the sklearn.preprocessing module to \n",
    "# standardize these scores\n",
    "std_sc = StandardScaler()\n",
    "s_data_1 = student_data.drop('ice_cream_flavor', axis = 'columns')\n",
    "print(s_data_1)\n",
    "\n",
    "std_data = std_sc.fit(s_data_1).transform(s_data_1)\n",
    "print(std_data.std(axis=0))\n",
    "\n",
    "# Merge the standardized scores to the dummy variables, and call the resulting DataFrame student_data_std. \n",
    "# student_data_std = std_data.append(dummy_vars)\n",
    "\n",
    "# # Print out the entire DataFrame. \n",
    "# print(student_data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    math  reading Dummy Vars\n",
      "0     90       82  [0, 0, 1]\n",
      "1    100       73  [1, 0, 0]\n",
      "2     80       92  [0, 0, 1]\n",
      "3     95      100  [0, 1, 0]\n",
      "4     90       79  [0, 1, 0]\n",
      "5     90       90  [1, 0, 0]\n",
      "6    100       89  [0, 0, 1]\n",
      "7     80       85  [0, 0, 1]\n",
      "8     85       99  [1, 0, 0]\n",
      "9    100       73  [1, 0, 0]\n",
      "10    80       92  [0, 0, 1]\n",
      "0     90\n",
      "1     92\n",
      "2     95\n",
      "3     82\n",
      "4     81\n",
      "5     90\n",
      "6     91\n",
      "7     79\n",
      "8     85\n",
      "9     92\n",
      "10    95\n",
      "Name: science, dtype: int64\n",
      "    math  reading Dummy Vars\n",
      "12    90       79  [0, 1, 0]\n",
      "13    86       92  [1, 0, 0]\n",
      "14   100       89  [0, 0, 1]\n",
      "15    83       95  [0, 0, 1]\n",
      "12    81\n",
      "13    90\n",
      "14    89\n",
      "15    79\n",
      "Name: science, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using a split ratio of 70:30, spit the student_data_1 into training and test set. \n",
    "# Reference the input and output of the training set as X_train and y_train respectively. \n",
    "# Reference the input and output of the test set as X_test and y_test respectively.\n",
    "X = student_data_1.drop('science', axis = 'columns')\n",
    "y = student_data_1.science\n",
    "\n",
    "X_train = X[:11]\n",
    "y_train = y[:11]\n",
    "X_test = X[12:]\n",
    "y_test = y[12:]\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12345679 0.12698413 0.17857143 0.125      0.07479675 0.03993344]\n",
      " [0.13580247 0.19047619 0.125      0.18055556 0.20650407 0.12479201]\n",
      " [0.34567901 0.50793651 0.32142857 0.30555556 0.36747967 0.34775374]\n",
      " ...\n",
      " [0.7037037  0.65079365 0.60714286 0.68055556 0.6796748  0.59733777]\n",
      " [0.50617284 0.41269841 0.375      0.43055556 0.44878049 0.37271215]\n",
      " [0.44444444 0.38095238 0.30357143 0.36111111 0.40487805 0.27620632]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=1140, minmax=(array([0., 0., 0., 0., 0., 0.]), array([1., 1., 1., 1., 1., 1.])), mean=array([0.59960372, 0.58755332, 0.52962018, 0.55377493, 0.58067372,\n",
       "       0.53106228]), variance=array([0.05415836, 0.05829545, 0.05775097, 0.05848488, 0.06123414,\n",
       "       0.07213077]), skewness=array([-0.17533383, -0.33477989, -0.13265523, -0.07365291, -0.06295492,\n",
       "       -0.05675943]), kurtosis=array([-1.14860356, -1.11636826, -1.25430314, -1.16092836, -1.27216709,\n",
       "       -1.3273628 ]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the boston_1 DataFrame in question 1a, select the LowTemp, HighTemp, WarmestMin, ColdestHigh, AveMin, AveMax columns\n",
    "# Use the “pipeline” functionality in sklearn to transform the selected data using the MinMaxScalar() & Imputer() classes.  \n",
    "bostonData = combined_boston.drop(['Month', 'Year'], axis = 'columns')\n",
    "\n",
    "# With the SimpleImputer() class, the missing data in each column should be imputed using the mean value for the column.\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n",
    "scaler = MinMaxScaler()\n",
    "pipe = Pipeline([(\"impute\", imp), (\"scaler\", scaler)])\n",
    "pipeline_data = pipe.fit_transform(bostonData)\n",
    "\n",
    "# Print the pipeline_data\n",
    "print(pipeline_data)\n",
    "\n",
    "# Output the descriptive statistics of the pipeline_data including mean, median, variance, minimum value, maximum value, \n",
    "# variance, standard deviation and skewness. You can use the .apply() functionality of the pandas DataFrame.\n",
    "scipy.stats.describe(pipeline_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
